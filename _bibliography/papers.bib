@InProceedings{chen2024urdformer,
  author    = {Chen, Zoey and Walsman, Aaron and Memmel, Marius and Mo, Kaichun and Fang, Alex  and  Vemuri, Karthikeya and Wu, Alan and Fox, Dieter and Gupta, Abhishek},
  booktitle = {Robotics: Science and Systems (RSS)},
  title     = {URDFormer: Constructing Interactive Realistic Scenes from Real Images via Simulation and Generative Modeling},
  year      = {2024},
  abstract  = {Constructing accurate and targeted simulation scenes that are both visually and physically
  realistic is a significant practical interest in domains ranging from robotics to computer vision.
  However, this process is typically done largely by hand - a graphic designer and a simulation engineer work together
  with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
  While this may scale to small numbers of scenes, to achieve the generalization properties that are requisite of data-driven machine
  learning algorithms, we require a pipeline that is able to synthesize large numbers of realistic scenes, c
  omplete with “natural” kinematic and dynamic structure. To do so, we develop models for inferring structure and generating simulation scenes
  from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models,
  we show how effective generative models can be used in generating training data, the network can be inverted to map from realistic images back to
  complete scene models. We show how this paradigm allows us to build large datasets of scenes with semantic and physical realism,
  enabling a variety of downstream applications in robotics and computer vision. },
  website       = {https://urdformer.github.io},
  code = {https://github.com/WEIRDLabUW/urdformer},
  selected={true},
  paper       = {2405.11656},
  abbr      = {RSS},
  note={Oral Presentation at CoRL TGR workshop},
  preview={/assets/img/publication_preview/rss_urdformer.gif}
}

@InProceedings{khazatsky2024droid,
    title   = {DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset},
    author  = {Alexander Khazatsky and Karl Pertsch and Suraj Nair and Ashwin Balakrishna and Sudeep Dasari and Siddharth Karamcheti and Soroush Nasiriany and Mohan Kumar Srirama and Lawrence Yunliang Chen and Kirsty Ellis and Peter David Fagan and Joey Hejna and Masha Itkina and Marion Lepert and Yecheng Jason Ma and Patrick Tree Miller and Jimmy Wu and Suneel Belkhale and Shivin Dass and Huy Ha and Arhan Jain and Abraham Lee and Youngwoon Lee and Marius Memmel and Sungjae Park and Ilija Radosavovic and Kaiyuan Wang and Albert Zhan and Kevin Black and Cheng Chi and Kyle Beltran Hatch and Shan Lin and Jingpei Lu and Jean Mercat and Abdul Rehman and Pannag R Sanketi and Archit Sharma and Cody Simpson and Quan Vuong and Homer Rich Walke and Blake Wulfe and Ted Xiao and Jonathan Heewon Yang and Arefeh Yavary and Tony Z. Zhao and Christopher Agia and Rohan Baijal and Mateo Guaman Castro and Daphne Chen and Qiuyu Chen and Trinity Chung and Jaimyn Drake and Ethan Paul Foster and Jensen Gao and David Antonio Herrera and Minho Heo and Kyle Hsu and Jiaheng Hu and Donovon Jackson and Charlotte Le and Yunshuang Li and Kevin Lin and Roy Lin and Zehan Ma and Abhiram Maddukuri and Suvir Mirchandani and Daniel Morton and Tony Nguyen and Abigail O'Neill and Rosario Scalise and Derick Seale and Victor Son and Stephen Tian and Emi Tran and Andrew E. Wang and Yilin Wu and Annie Xie and Jingyun Yang and Patrick Yin and Yunchu Zhang and Osbert Bastani and Glen Berseth and Jeannette Bohg and Ken Goldberg and Abhinav Gupta and Abhishek Gupta and Dinesh Jayaraman and Joseph J Lim and Jitendra Malik and Roberto Martín-Martín and Subramanian Ramamoorthy and Dorsa Sadigh and Shuran Song and Jiajun Wu and Michael C. Yip and Yuke Zhu and Thomas Kollar and Sergey Levine and Chelsea Finn},
    booktitle = {Robotics: Science and Systems (RSS)},
    website       = {https://droid-dataset.github.io/},
    abstract  = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path
    toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging:
    collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial
    investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on
    data collected in a small number of environments with limited scene and task diversity. In this work, we introduce
    DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or
    350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America,
    Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance,
    greater robustness, and improved generalization ability. We open source the full dataset, code for policy training,
    and a detailed guide for reproducing our robot hardware setup.},
    abbr      = {RSS},
    year    = {2024},
    selected={false},
    paper       = {2403.12945},
    preview={/assets/img/publication_preview/droid.png}
}
@misc{open_x_embodiment_rt_x_2023,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {Open X-Embodiment Collaboration and Abby O'Neill and Abdul Rehman and Abhiram Maddukuri and Abhishek Gupta and Abhishek Padalkar and Abraham Lee and Acorn Pooley and Agrim Gupta and Ajay Mandlekar and Ajinkya Jain and Albert Tung and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anchit Gupta and Andrew Wang and Andrey Kolobov and Anikait Singh and Animesh Garg and Aniruddha Kembhavi and Annie Xie and Anthony Brohan and Antonin Raffin and Archit Sharma and Arefeh Yavary and Arhan Jain and Ashwin Balakrishna and Ayzaan Wahid and Ben Burgess-Limerick and Beomjoon Kim and Bernhard Schölkopf and Blake Wulfe and Brian Ichter and Cewu Lu and Charles Xu and Charlotte Le and Chelsea Finn and Chen Wang and Chenfeng Xu and Cheng Chi and Chenguang Huang and Christine Chan and Christopher Agia and Chuer Pan and Chuyuan Fu and Coline Devin and Danfei Xu and Daniel Morton and Danny Driess and Daphne Chen and Deepak Pathak and Dhruv Shah and Dieter Büchler and Dinesh Jayaraman and Dmitry Kalashnikov and Dorsa Sadigh and Edward Johns and Ethan Foster and Fangchen Liu and Federico Ceola and Fei Xia and Feiyu Zhao and Felipe Vieira Frujeri and Freek Stulp and Gaoyue Zhou and Gaurav S. Sukhatme and Gautam Salhotra and Ge Yan and Gilbert Feng and Giulio Schiavi and Glen Berseth and Gregory Kahn and Guangwen Yang and Guanzhi Wang and Hao Su and Hao-Shu Fang and Haochen Shi and Henghui Bao and Heni Ben Amor and Henrik I Christensen and Hiroki Furuta and Homer Walke and Hongjie Fang and Huy Ha and Igor Mordatch and Ilija Radosavovic and Isabel Leal and Jacky Liang and Jad Abou-Chakra and Jaehyung Kim and Jaimyn Drake and Jan Peters and Jan Schneider and Jasmine Hsu and Jeannette Bohg and Jeffrey Bingham and Jeffrey Wu and Jensen Gao and Jiaheng Hu and Jiajun Wu and Jialin Wu and Jiankai Sun and Jianlan Luo and Jiayuan Gu and Jie Tan and Jihoon Oh and Jimmy Wu and Jingpei Lu and Jingyun Yang and Jitendra Malik and João Silvério and Joey Hejna and Jonathan Booher and Jonathan Tompson and Jonathan Yang and Jordi Salvador and Joseph J. Lim and Junhyek Han and Kaiyuan Wang and Kanishka Rao and Karl Pertsch and Karol Hausman and Keegan Go and Keerthana Gopalakrishnan and Ken Goldberg and Kendra Byrne and Kenneth Oslund and Kento Kawaharazuka and Kevin Black and Kevin Lin and Kevin Zhang and Kiana Ehsani and Kiran Lekkala and Kirsty Ellis and Krishan Rana and Krishnan Srinivasan and Kuan Fang and Kunal Pratap Singh and Kuo-Hao Zeng and Kyle Hatch and Kyle Hsu and Laurent Itti and Lawrence Yunliang Chen and Lerrel Pinto and Li Fei-Fei and Liam Tan and Linxi "Jim" Fan and Lionel Ott and Lisa Lee and Luca Weihs and Magnum Chen and Marion Lepert and Marius Memmel and Masayoshi Tomizuka and Masha Itkina and Mateo Guaman Castro and Max Spero and Maximilian Du and Michael Ahn and Michael C. Yip and Mingtong Zhang and Mingyu Ding and Minho Heo and Mohan Kumar Srirama and Mohit Sharma and Moo Jin Kim and Naoaki Kanazawa and Nicklas Hansen and Nicolas Heess and Nikhil J Joshi and Niko Suenderhauf and Ning Liu and Norman Di Palo and Nur Muhammad Mahi Shafiullah and Oier Mees and Oliver Kroemer and Osbert Bastani and Pannag R Sanketi and Patrick "Tree" Miller and Patrick Yin and Paul Wohlhart and Peng Xu and Peter David Fagan and Peter Mitrano and Pierre Sermanet and Pieter Abbeel and Priya Sundaresan and Qiuyu Chen and Quan Vuong and Rafael Rafailov and Ran Tian and Ria Doshi and Roberto Mart{'i}n-Mart{'i}n and Rohan Baijal and Rosario Scalise and Rose Hendrix and Roy Lin and Runjia Qian and Ruohan Zhang and Russell Mendonca and Rutav Shah and Ryan Hoque and Ryan Julian and Samuel Bustamante and Sean Kirmani and Sergey Levine and Shan Lin and Sherry Moore and Shikhar Bahl and Shivin Dass and Shubham Sonawani and Shuran Song and Sichun Xu and Siddhant Haldar and Siddharth Karamcheti and Simeon Adebola and Simon Guist and Soroush Nasiriany and Stefan Schaal and Stefan Welker and Stephen Tian and Subramanian Ramamoorthy and Sudeep Dasari and Suneel Belkhale and Sungjae Park and Suraj Nair and Suvir Mirchandani and Takayuki Osa and Tanmay Gupta and Tatsuya Harada and Tatsuya Matsushima and Ted Xiao and Thomas Kollar and Tianhe Yu and Tianli Ding and Todor Davchev and Tony Z. Zhao and Travis Armstrong and Trevor Darrell and Trinity Chung and Vidhi Jain and Vincent Vanhoucke and Wei Zhan and Wenxuan Zhou and Wolfram Burgard and Xi Chen and Xiangyu Chen and Xiaolong Wang and Xinghao Zhu and Xinyang Geng and Xiyuan Liu and Xu Liangwei and Xuanlin Li and Yansong Pang and Yao Lu and Yecheng Jason Ma and Yejin Kim and Yevgen Chebotar and Yifan Zhou and Yifeng Zhu and Yilin Wu and Ying Xu and Yixuan Wang and Yonatan Bisk and Yongqiang Dou and Yoonyoung Cho and Youngwoon Lee and Yuchen Cui and Yue Cao and Yueh-Hua Wu and Yujin Tang and Yuke Zhu and Yunchu Zhang and Yunfan Jiang and Yunshuang Li and Yunzhu Li and Yusuke Iwasawa and Yutaka Matsuo and Zehan Ma and Zhuo Xu and Zichen Jeff Cui and Zichen Zhang and Zipeng Fu and Zipeng Lin},
year = {2023},
abstract  = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.},
  website       = {https://robotics-transformer-x.github.io/},
  selected={false},
  paper       = {2310.08864},
  preview={/assets/img/publication_preview/openx.png}
}

@InProceedings{chen2023genaug,
  author    = {Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  booktitle = {Robotics: Science and Systems (RSS)},
  title     = {GenAug: Retargeting Behaviors to Unseen Situations via Generative Augmentation},
  year      = {2023},
  abstract  = {Robot learning methods have the potential for widespread generalization across tasks, environments,
  and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings.
  For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience.
  In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data,
  can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data,
  they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization.
   In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation.
   By leveraging these pre-trained models for generating appropriate “semantic” data augmentations, we propose a system GenAug that is able to significantly
   improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios,
   while only requiring marginal amounts of real-world data. },
  website       = {https://genaug.github.io/},
  code = {https://github.com/genaug/genaug},
  poster = {genaug_poster.pdf},
  paper       = {2302.06671},
  abbr      = {RSS},
  slides = {https://docs.google.com/presentation/d/1KJ4A6PI7hMb-ZbmH_dzrN8ORLRzIye8SOqVUM3ST8as/edit?usp=sharing},
  selected={true},
  note={Best System Paper Finalist},
  preview={/assets/img/publication_preview/genaug.gif}
}

@InProceedings{meng2023terrainnet,
  title= {TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation},
  author= {Meng, Xiangyun and Hatch, Nathan and Lambert, Alexander and Li, Anqi and Wagener, Nolan and Schmittle, Matthew and Lee, JoonHo and Yuan, Wentao and Chen, Zoey and Deng, Samuel and Okopal, Greg and Fox, Dieter and Boots, Byron and AShaban, mirreza},
  booktitle = {Robotics: Science and Systems (RSS)},
  year      = {2023},
    abstract={Effective use of camera-based vision systems is
essential for robust performance in autonomous off-road driving,
particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene
prediction have yet to be successfully adapted for complex outdoor
terrain. To this end, we present TerrainNet, a vision-based terrain
perception system for semantic and geometric terrain prediction
for aggressive, off-road navigation. The approach relies on several
key insights and practical considerations for achieving reliable
terrain modeling. The network includes a multi-headed output
representation to capture fine- and coarse-grained terrain features
necessary for estimating traversability. Accurate depth estimation
is achieved using self-supervised depth completion with multi-view
RGB and stereo inputs. Requirements for real-time performance
and fast inference speeds are met using efficient, learned image
feature projections. },
  website       = {https://sites.google.com/view/visual-terrain-modeling},
  paper       = {2303.15771},
  abbr      = {RSS},
  selected={false},
  preview={/assets/img/publication_preview/racer.jpg}
}

@InProceedings{chen2022learning,
  author    = {Chen, Zoey Qiuyu and Van Wyk, Karl and Chao, Yu-Wei and Yang, Wei and Mousavian, Arsalan and Gupta, Abhishek and Fox, Dieter},
  booktitle = {Conference on Robot Learning (CoRL)},
  title     = {Learning Robust Real-world Dexterous Grasping Policies via Implicit Shape Augmentation},
  year      = {2022},
  abstract  = {Dexterous robotic hands have the capability to interact with a wide
variety of household objects to perform tasks like grasping. However, learning robust real world grasping policies for arbitrary objects has proven challenging due
to the difficulty of generating high quality training data. In this work, we propose
a learning system (ISAGrasp) for leveraging a small number of human demonstrations to bootstrap the generation of a much larger dataset containing successful
grasps on a variety of novel objects. Our key insight is to use a correspondenceaware implicit generative model to deform object meshes and demonstrated human grasps in order to generate a diverse dataset of novel objects and successful
grasps for supervised learning, while maintaining semantic realism. We use this
dataset to train a robust grasping policy in simulation which can be deployed in
the real world. },
  website       = {https://sites.google.com/view/implicitaugmentation/home},
  paper       = {2210.13638},
  selected={true},
  abbr      = {CoRL},
  preview={/assets/img/publication_preview/isagrasp.gif}
}

@InProceedings{chen2022learning,
  author    = {Chen, Zoey Qiuyu and Van Wyk, Karl and Chao, Yu-Wei and Yang, Wei and Mousavian, Arsalan and Gupta, Abhishek and Fox, Dieter},
  booktitle = {Robotics: Science and Systems (RSS) workshop},
  title     = {DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations},
  year      = {2022},
  abstract = {Teaching a multi-fingered dexterous robot to grasp objects in the real world has been a challenging problem due to its high dimensional state and action space. We propose a robot-learning system that can take a small number of human demonstrations and learn to grasp unseen object poses given partially occluded observations. Our system leverages a small motion capture dataset and generates a large dataset with diverse and successful trajectories for a multi-fingered robot gripper. By adding domain randomization, we show that our dataset provides robust grasping trajectories that can be transferred to a policy learner. We train a dexterous grasping policy that takes the point clouds of the object as input and predicts continuous actions to grasp objects from different initial robot states. We evaluate the effectiveness of our system on a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand with a KUKA arm in the real world. The policy learned from our dataset can generalize well on unseen object poses in both simulation and the real world.},
  website       = {https://sites.google.com/view/dextransfer/home},
  paper       = {2209.14284},
  selected={false},
  note={Spotlight},
  abbr      = {RSS workshop},
  preview={/assets/img/publication_preview/dextransfer.png}
}
