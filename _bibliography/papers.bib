@InProceedings{chen2024urdformer,
  author    = {Chen, Zoey and Walsman, Aaron and Memmel, Marius and Mo, Kaichun and Fang, Alex  and  Vemuri, Karthikeya and Wu, Alan and Fox, Dieter and Gupta, Abhishek},
  booktitle = {Robotics: Science and Systems (RSS)},
  title     = {URDFormer: Constructing Interactive Realistic Scenes from Real Images via Simulation and Generative Modeling},
  year      = {2024},
  abstract  = {Constructing accurate and targeted simulation scenes that are both visually and physically
  realistic is a significant practical interest in domains ranging from robotics to computer vision.
  However, this process is typically done largely by hand - a graphic designer and a simulation engineer work together
  with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
  While this may scale to small numbers of scenes, to achieve the generalization properties that are requisite of data-driven machine
  learning algorithms, we require a pipeline that is able to synthesize large numbers of realistic scenes, c
  omplete with “natural” kinematic and dynamic structure. To do so, we develop models for inferring structure and generating simulation scenes
  from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models,
  we show how effective generative models can be used in generating training data, the network can be inverted to map from realistic images back to
  complete scene models. We show how this paradigm allows us to build large datasets of scenes with semantic and physical realism,
  enabling a variety of downstream applications in robotics and computer vision. },
  website       = {https://urdformer.github.io},
  code = {https://github.com/WEIRDLabUW/urdformer},
  selected={true},
  paper       = {2405.11656},
  abbr      = {RSS},
  note={Oral Presentation at CoRL TGR workshop},
  preview={/assets/img/publication_preview/rss_urdformer.gif}
}

@InProceedings{khazatsky2024droid,
    title   = {DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset},
    author  = {Alexander Khazatsky and Karl Pertsch and Suraj Nair and Ashwin Balakrishna and Sudeep Dasari and Siddharth Karamcheti and Soroush Nasiriany and Mohan Kumar Srirama and Lawrence Yunliang Chen and Kirsty Ellis and Peter David Fagan and Joey Hejna and Masha Itkina and Marion Lepert and Yecheng Jason Ma and Patrick Tree Miller and Jimmy Wu and Suneel Belkhale and Shivin Dass and Huy Ha and Arhan Jain and Abraham Lee and Youngwoon Lee and Marius Memmel and Sungjae Park and Ilija Radosavovic and Kaiyuan Wang and Albert Zhan and Kevin Black and Cheng Chi and Kyle Beltran Hatch and Shan Lin and Jingpei Lu and Jean Mercat and Abdul Rehman and Pannag R Sanketi and Archit Sharma and Cody Simpson and Quan Vuong and Homer Rich Walke and Blake Wulfe and Ted Xiao and Jonathan Heewon Yang and Arefeh Yavary and Tony Z. Zhao and Christopher Agia and Rohan Baijal and Mateo Guaman Castro and Daphne Chen and Qiuyu Chen and Trinity Chung and Jaimyn Drake and Ethan Paul Foster and Jensen Gao and David Antonio Herrera and Minho Heo and Kyle Hsu and Jiaheng Hu and Donovon Jackson and Charlotte Le and Yunshuang Li and Kevin Lin and Roy Lin and Zehan Ma and Abhiram Maddukuri and Suvir Mirchandani and Daniel Morton and Tony Nguyen and Abigail O'Neill and Rosario Scalise and Derick Seale and Victor Son and Stephen Tian and Emi Tran and Andrew E. Wang and Yilin Wu and Annie Xie and Jingyun Yang and Patrick Yin and Yunchu Zhang and Osbert Bastani and Glen Berseth and Jeannette Bohg and Ken Goldberg and Abhinav Gupta and Abhishek Gupta and Dinesh Jayaraman and Joseph J Lim and Jitendra Malik and Roberto Martín-Martín and Subramanian Ramamoorthy and Dorsa Sadigh and Shuran Song and Jiajun Wu and Michael C. Yip and Yuke Zhu and Thomas Kollar and Sergey Levine and Chelsea Finn},
    booktitle = {Robotics: Science and Systems (RSS)},
    website       = {https://droid-dataset.github.io/},
    abstract  = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path
    toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging:
    collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial
    investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on
    data collected in a small number of environments with limited scene and task diversity. In this work, we introduce
    DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or
    350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America,
    Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance,
    greater robustness, and improved generalization ability. We open source the full dataset, code for policy training,
    and a detailed guide for reproducing our robot hardware setup.},
    abbr      = {RSS},
    year    = {2024},
    selected={false},
    paper       = {2403.12945},
    preview={/assets/img/publication_preview/droid.png}
}

@InProceedings{chen2023genaug,
  author    = {Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  booktitle = {Robotics: Science and Systems (RSS)},
  title     = {GenAug: Retargeting Behaviors to Unseen Situations via Generative Augmentation},
  year      = {2023},
  abstract  = {Robot learning methods have the potential for widespread generalization across tasks, environments,
  and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings.
  For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience.
  In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data,
  can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data,
  they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization.
   In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation.
   By leveraging these pre-trained models for generating appropriate “semantic” data augmentations, we propose a system GenAug that is able to significantly
   improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios,
   while only requiring marginal amounts of real-world data. },
  website       = {https://genaug.github.io/},
  code = {https://github.com/genaug/genaug},
  poster = {genaug_poster.pdf},
  paper       = {2302.06671},
  abbr      = {RSS},
  slides = {https://docs.google.com/presentation/d/1KJ4A6PI7hMb-ZbmH_dzrN8ORLRzIye8SOqVUM3ST8as/edit?usp=sharing},
  selected={true},
  note={Best System Paper Finalist},
  preview={/assets/img/publication_preview/genaug.gif}
}

@InProceedings{meng2023terrainnet,
  title= {TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation},
  author= {Meng, Xiangyun and Hatch, Nathan and Lambert, Alexander and Li, Anqi and Wagener, Nolan and Schmittle, Matthew and Lee, JoonHo and Yuan, Wentao and Chen, Zoey and Deng, Samuel and Okopal, Greg and Fox, Dieter and Boots, Byron and AShaban, mirreza},
  booktitle = {Robotics: Science and Systems (RSS)},
  year      = {2023},
    abstract={Effective use of camera-based vision systems is
essential for robust performance in autonomous off-road driving,
particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene
prediction have yet to be successfully adapted for complex outdoor
terrain. To this end, we present TerrainNet, a vision-based terrain
perception system for semantic and geometric terrain prediction
for aggressive, off-road navigation. The approach relies on several
key insights and practical considerations for achieving reliable
terrain modeling. The network includes a multi-headed output
representation to capture fine- and coarse-grained terrain features
necessary for estimating traversability. Accurate depth estimation
is achieved using self-supervised depth completion with multi-view
RGB and stereo inputs. Requirements for real-time performance
and fast inference speeds are met using efficient, learned image
feature projections. },
  website       = {https://sites.google.com/view/visual-terrain-modeling},
  paper       = {2303.15771},
  abbr      = {RSS},
  selected={false},
  preview={/assets/img/publication_preview/racer.jpg}
}

@InProceedings{chen2022learning,
  author    = {Chen, Zoey Qiuyu and Van Wyk, Karl and Chao, Yu-Wei and Yang, Wei and Mousavian, Arsalan and Gupta, Abhishek and Fox, Dieter},
  booktitle = {Conference on Robot Learning (CoRL)},
  title     = {Learning Robust Real-world Dexterous Grasping Policies via Implicit Shape Augmentation},
  year      = {2022},
  abstract  = {Dexterous robotic hands have the capability to interact with a wide
variety of household objects to perform tasks like grasping. However, learning robust real world grasping policies for arbitrary objects has proven challenging due
to the difficulty of generating high quality training data. In this work, we propose
a learning system (ISAGrasp) for leveraging a small number of human demonstrations to bootstrap the generation of a much larger dataset containing successful
grasps on a variety of novel objects. Our key insight is to use a correspondenceaware implicit generative model to deform object meshes and demonstrated human grasps in order to generate a diverse dataset of novel objects and successful
grasps for supervised learning, while maintaining semantic realism. We use this
dataset to train a robust grasping policy in simulation which can be deployed in
the real world. },
  website       = {https://sites.google.com/view/implicitaugmentation/home},
  paper       = {2210.13638},
  selected={true},
  abbr      = {CoRL},
  preview={/assets/img/publication_preview/isagrasp.gif}
}

@InProceedings{chen2022learning,
  author    = {Chen, Zoey Qiuyu and Van Wyk, Karl and Chao, Yu-Wei and Yang, Wei and Mousavian, Arsalan and Gupta, Abhishek and Fox, Dieter},
  booktitle = {Robotics: Science and Systems (RSS) workshop},
  title     = {DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations},
  year      = {2022},
  abstract = {Teaching a multi-fingered dexterous robot to grasp objects in the real world has been a challenging problem due to its high dimensional state and action space. We propose a robot-learning system that can take a small number of human demonstrations and learn to grasp unseen object poses given partially occluded observations. Our system leverages a small motion capture dataset and generates a large dataset with diverse and successful trajectories for a multi-fingered robot gripper. By adding domain randomization, we show that our dataset provides robust grasping trajectories that can be transferred to a policy learner. We train a dexterous grasping policy that takes the point clouds of the object as input and predicts continuous actions to grasp objects from different initial robot states. We evaluate the effectiveness of our system on a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand with a KUKA arm in the real world. The policy learned from our dataset can generalize well on unseen object poses in both simulation and the real world.},
  website       = {https://sites.google.com/view/dextransfer/home},
  paper       = {2209.14284},
  selected={false},
  note={Spotlight},
  abbr      = {RSS workshop},
  preview={/assets/img/publication_preview/dextransfer.png}
}
