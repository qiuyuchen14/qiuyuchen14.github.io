@Article{chen2025semantically,
  author  = {Chen, Zoey and Mandi, Zhao and Bharadhwaj, Homanga and Sharma, Mohit and Song, Shuran and Gupta, Abhishek and Kumar, Vikash},
  title   = {Semantically Controllable Augmentations for Generalizable Robot Learning},
  journal = {The International Journal of Robotics Research (IJRR)},
  year    = {2025},
  volume  = {44},
  number  = {10-11},
  pages   = {1705-1726},
  abbr    = {IJRR},
  abstract = {Generalization to unseen real-world scenarios for robot manipulation requires exposure to diverse datasets during training. However, collecting large real-world datasets is intractable due to high operational costs. For robot learning to generalize despite these challenges, it is essential to leverage sources of data or priors beyond the robot's direct experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. These generative models encompass a broad range of real-world scenarios beyond a robot's direct experience and can synthesize novel synthetic experiences that expose robotic agents to additional world priors aiding real-world generalization at no extra cost. In particular, our approach leverages pre-trained generative models as an effective tool for data augmentation. We propose a generative augmentation framework for semantically controllable augmentations and rapidly multiplying robot datasets while inducing rich variations that enable real-world generalization. Based on diverse augmentations of robot data, we show how scalable robot manipulation policies can be trained and deployed both in simulation and in unseen real-world environments such as kitchens and table-tops. By demonstrating the effectiveness of image-text generative models in diverse real-world robotic applications, our generative augmentation framework provides a scalable and efficient path for boosting generalization in robot learning at no extra human cost.},
  paper   = {2409.00951},
  preview = {/assets/img/publication_preview/ijrr.jpg},
  selected={false}
}

@InProceedings{vemuri2025duolingo,
  author    = {Vemuri, Karthikeya and Wu, Alan and Thareja, Arnav and Chen, Zoey and Good, Ian and Lipton, Jeffrey and Gupta, Abhishek},
  title     = {Duolingo: Dynamics Utilization for Online Translation of Actions},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2025},
  pages     = {1451-1458},
  abbr      = {ICRA},
  abstract  = {Robots in the real world experience wear and tear, leading to changing system dynamics. This challenge is particularly exacerbated for non-rigid systems such as soft robots or robotic systems made of metamaterials with hysteresis. This setting results in a challenging problem for most learning-based controllers that typically rely on the assumption that the system dynamics remain fixed over time. In the absence of explicit mechanisms to account for this change in dynamics, learning-based control algorithms show considerable degradation in performance over time. In this work, we consider a particular class of dynamics shift in under-actuated systems, that is localized to the dynamics of the fully actuated robot itself, while independently leaving the dynamics of the environment unchanged. This captures real-world phenomena such as fatigue or hysteresis in robotic systems. In this setting, we propose an efficient algorithm that can account for dynamics shift. Using a simple calibration procedure, we propose a technique for learning a non-linear “action-translation” model that can capture the localized shift in dynamics. This enables continual learning and transfer despite considerable dynamics shift during the learning process. We demonstrate the efficacy of this procedure on several tasks in simulation, as well as a real-world robotic system - a 4 DoF electrically driven handed shearing auxetic (HSA) platform.},
  paper     = {https://ieeexplore.ieee.org/abstract/document/11127601},
  preview   = {/assets/img/publication_preview/duolingo.png},
  selected={false}
}

@InProceedings{chen2024urdformer,
  author    = {Chen, Zoey and Walsman, Aaron and Memmel, Marius and Mo, Kaichun and Fang, Alex  and  Vemuri, Karthikeya and Wu, Alan and Fox, Dieter and Gupta, Abhishek},
  booktitle = {Robotics: Science and Systems (RSS)},
  title     = {URDFormer: Constructing Interactive Realistic Scenes from Real Images via Simulation and Generative Modeling},
  year      = {2024},
  abstract  = {Constructing accurate and targeted simulation scenes that are both visually and physically
  realistic is a significant practical interest in domains ranging from robotics to computer vision.
  However, this process is typically done largely by hand - a graphic designer and a simulation engineer work together
  with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
  While this may scale to small numbers of scenes, to achieve the generalization properties that are requisite of data-driven machine
  learning algorithms, we require a pipeline that is able to synthesize large numbers of realistic scenes, c
  omplete with “natural” kinematic and dynamic structure. To do so, we develop models for inferring structure and generating simulation scenes
  from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models,
  we show how effective generative models can be used in generating training data, the network can be inverted to map from realistic images back to
  complete scene models. We show how this paradigm allows us to build large datasets of scenes with semantic and physical realism,
  enabling a variety of downstream applications in robotics and computer vision. },
  website       = {https://urdformer.github.io},
  code = {https://github.com/WEIRDLabUW/urdformer},
  selected={true},
  paper       = {2405.11656},
  abbr      = {RSS},
  note={Oral Presentation at CoRL TGR workshop},
  preview={/assets/img/publication_preview/rss_urdformer.gif}
}

@InProceedings{khazatsky2024droid,
    title   = {DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset},
    author  = {Alexander Khazatsky and Karl Pertsch and Suraj Nair and Ashwin Balakrishna and Sudeep Dasari and Siddharth Karamcheti and Soroush Nasiriany and Mohan Kumar Srirama and Lawrence Yunliang Chen and Kirsty Ellis and Peter David Fagan and Joey Hejna and Masha Itkina and Marion Lepert and Yecheng Jason Ma and Patrick Tree Miller and Jimmy Wu and Suneel Belkhale and Shivin Dass and Huy Ha and Arhan Jain and Abraham Lee and Youngwoon Lee and Marius Memmel and Sungjae Park and Ilija Radosavovic and Kaiyuan Wang and Albert Zhan and Kevin Black and Cheng Chi and Kyle Beltran Hatch and Shan Lin and Jingpei Lu and Jean Mercat and Abdul Rehman and Pannag R Sanketi and Archit Sharma and Cody Simpson and Quan Vuong and Homer Rich Walke and Blake Wulfe and Ted Xiao and Jonathan Heewon Yang and Arefeh Yavary and Tony Z. Zhao and Christopher Agia and Rohan Baijal and Mateo Guaman Castro and Daphne Chen and Qiuyu Chen and Trinity Chung and Jaimyn Drake and Ethan Paul Foster and Jensen Gao and David Antonio Herrera and Minho Heo and Kyle Hsu and Jiaheng Hu and Donovon Jackson and Charlotte Le and Yunshuang Li and Kevin Lin and Roy Lin and Zehan Ma and Abhiram Maddukuri and Suvir Mirchandani and Daniel Morton and Tony Nguyen and Abigail O'Neill and Rosario Scalise and Derick Seale and Victor Son and Stephen Tian and Emi Tran and Andrew E. Wang and Yilin Wu and Annie Xie and Jingyun Yang and Patrick Yin and Yunchu Zhang and Osbert Bastani and Glen Berseth and Jeannette Bohg and Ken Goldberg and Abhinav Gupta and Abhishek Gupta and Dinesh Jayaraman and Joseph J Lim and Jitendra Malik and Roberto Martín-Martín and Subramanian Ramamoorthy and Dorsa Sadigh and Shuran Song and Jiajun Wu and Michael C. Yip and Yuke Zhu and Thomas Kollar and Sergey Levine and Chelsea Finn},
    booktitle = {Robotics: Science and Systems (RSS)},
    website       = {https://droid-dataset.github.io/},
    abstract  = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path
    toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging:
    collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial
    investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on
    data collected in a small number of environments with limited scene and task diversity. In this work, we introduce
    DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or
    350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America,
    Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance,
    greater robustness, and improved generalization ability. We open source the full dataset, code for policy training,
    and a detailed guide for reproducing our robot hardware setup.},
    abbr      = {RSS},
    year    = {2024},
    selected={false},
    paper       = {2403.12945},
    preview={/assets/img/publication_preview/droid.png}
}
@InProceedings{open_x_embodiment_rt_x_2023,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {Open X-Embodiment Collaboration},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
year = {2024},
abstract  = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.},
  website       = {https://robotics-transformer-x.github.io/},
  note={Best Conference Paper Award},
  selected={false},
  paper       = {2310.08864},
  preview={/assets/img/publication_preview/openx.png}
}

@InProceedings{chen2023genaug,
  author    = {Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  booktitle = {Robotics: Science and Systems (RSS)},
  title     = {GenAug: Retargeting Behaviors to Unseen Situations via Generative Augmentation},
  year      = {2023},
  abstract  = {Robot learning methods have the potential for widespread generalization across tasks, environments,
  and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings.
  For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience.
  In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data,
  can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data,
  they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization.
   In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation.
   By leveraging these pre-trained models for generating appropriate “semantic” data augmentations, we propose a system GenAug that is able to significantly
   improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios,
   while only requiring marginal amounts of real-world data. },
  website       = {https://genaug.github.io/},
  code = {https://github.com/genaug/genaug},
  poster = {genaug_poster.pdf},
  paper       = {2302.06671},
  abbr      = {RSS},
  slides = {https://docs.google.com/presentation/d/1KJ4A6PI7hMb-ZbmH_dzrN8ORLRzIye8SOqVUM3ST8as/edit?usp=sharing},
  selected={true},
  note={Best System Paper Finalist},
  preview={/assets/img/publication_preview/genaug.gif}
}

@InProceedings{meng2023terrainnet,
  title= {TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation},
  author= {Meng, Xiangyun and Hatch, Nathan and Lambert, Alexander and Li, Anqi and Wagener, Nolan and Schmittle, Matthew and Lee, JoonHo and Yuan, Wentao and Chen, Zoey and Deng, Samuel and Okopal, Greg and Fox, Dieter and Boots, Byron and AShaban, mirreza},
  booktitle = {Robotics: Science and Systems (RSS)},
  year      = {2023},
    abstract={Effective use of camera-based vision systems is
essential for robust performance in autonomous off-road driving,
particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene
prediction have yet to be successfully adapted for complex outdoor
terrain. To this end, we present TerrainNet, a vision-based terrain
perception system for semantic and geometric terrain prediction
for aggressive, off-road navigation. The approach relies on several
key insights and practical considerations for achieving reliable
terrain modeling. The network includes a multi-headed output
representation to capture fine- and coarse-grained terrain features
necessary for estimating traversability. Accurate depth estimation
is achieved using self-supervised depth completion with multi-view
RGB and stereo inputs. Requirements for real-time performance
and fast inference speeds are met using efficient, learned image
feature projections. },
  website       = {https://sites.google.com/view/visual-terrain-modeling},
  paper       = {2303.15771},
  abbr      = {RSS},
  selected={false},
  preview={/assets/img/publication_preview/racer.jpg}
}

@InProceedings{chen2022learning,
  author    = {Chen, Zoey Qiuyu and Van Wyk, Karl and Chao, Yu-Wei and Yang, Wei and Mousavian, Arsalan and Gupta, Abhishek and Fox, Dieter},
  booktitle = {Conference on Robot Learning (CoRL)},
  title     = {Learning Robust Real-world Dexterous Grasping Policies via Implicit Shape Augmentation},
  year      = {2022},
  abstract  = {Dexterous robotic hands have the capability to interact with a wide
variety of household objects to perform tasks like grasping. However, learning robust real world grasping policies for arbitrary objects has proven challenging due
to the difficulty of generating high quality training data. In this work, we propose
a learning system (ISAGrasp) for leveraging a small number of human demonstrations to bootstrap the generation of a much larger dataset containing successful
grasps on a variety of novel objects. Our key insight is to use a correspondenceaware implicit generative model to deform object meshes and demonstrated human grasps in order to generate a diverse dataset of novel objects and successful
grasps for supervised learning, while maintaining semantic realism. We use this
dataset to train a robust grasping policy in simulation which can be deployed in
the real world. },
  website       = {https://sites.google.com/view/implicitaugmentation/home},
  paper       = {2210.13638},
  selected={true},
  abbr      = {CoRL},
  preview={/assets/img/publication_preview/isagrasp.gif}
}

@InProceedings{chen2022learning,
  author    = {Chen, Zoey Qiuyu and Van Wyk, Karl and Chao, Yu-Wei and Yang, Wei and Mousavian, Arsalan and Gupta, Abhishek and Fox, Dieter},
  booktitle = {Robotics: Science and Systems (RSS) workshop},
  title     = {DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations},
  year      = {2022},
  abstract = {Teaching a multi-fingered dexterous robot to grasp objects in the real world has been a challenging problem due to its high dimensional state and action space. We propose a robot-learning system that can take a small number of human demonstrations and learn to grasp unseen object poses given partially occluded observations. Our system leverages a small motion capture dataset and generates a large dataset with diverse and successful trajectories for a multi-fingered robot gripper. By adding domain randomization, we show that our dataset provides robust grasping trajectories that can be transferred to a policy learner. We train a dexterous grasping policy that takes the point clouds of the object as input and predicts continuous actions to grasp objects from different initial robot states. We evaluate the effectiveness of our system on a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand with a KUKA arm in the real world. The policy learned from our dataset can generalize well on unseen object poses in both simulation and the real world.},
  website       = {https://sites.google.com/view/dextransfer/home},
  paper       = {2209.14284},
  selected={false},
  note={Spotlight},
  abbr      = {RSS workshop},
  preview={/assets/img/publication_preview/dextransfer.png}
}
