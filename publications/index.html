<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="" /><meta name="msvalidate.01" content="" />

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Zoey  Chen | Publications</title>
    <meta name="author" content="Zoey  Chen" />
    <meta name="description" content="Zoey's website.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img//assets/img/zoey.JPG"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://qiuyuchen14.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://qiuyuchen14.github.io/"><span class="font-weight-bold">Zoey</span>   Chen</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <div class="publications">

<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/rss_urdformer.gif" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2024urdformer" class="col-sm-7">
        <!-- Title -->
        <div class="title">URDFormer: Constructing Interactive Realistic Scenes from Real Images via Simulation and Generative Modeling</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Chen</em>, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Karthikeya Vemuri, Alan Wu, Dieter Fox, and Abhishek Gupta</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS)</em>, 2024
        </div>
        <div class="periodical">
          <span style="color: red;">
          Oral Presentation at CoRL TGR workshop
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2405.11656" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/WEIRDLabUW/urdformer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://urdformer.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Constructing accurate and targeted simulation scenes that are both visually and physically
  realistic is a significant practical interest in domains ranging from robotics to computer vision.
  However, this process is typically done largely by hand - a graphic designer and a simulation engineer work together
  with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
  While this may scale to small numbers of scenes, to achieve the generalization properties that are requisite of data-driven machine
  learning algorithms, we require a pipeline that is able to synthesize large numbers of realistic scenes, c
  omplete with “natural” kinematic and dynamic structure. To do so, we develop models for inferring structure and generating simulation scenes
  from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models,
  we show how effective generative models can be used in generating training data, the network can be inverted to map from realistic images back to
  complete scene models. We show how this paradigm allows us to build large datasets of scenes with semantic and physical realism,
  enabling a variety of downstream applications in robotics and computer vision. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/droid.png" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="khazatsky2024droid" class="col-sm-7">
        <!-- Title -->
        <div class="title">DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</div>
        <!-- Author -->
        <div class="author">
        

        Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, and
          <span class="more-authors" title="click to view 89 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '89 more authors' ? 'Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag R Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail O’Neill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, Chelsea Finn' : '89 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">89 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS)</em>, 2024
        </div>
        <div class="periodical">
          <span style="color: red;">
          
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2403.12945" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://droid-dataset.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path
    toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging:
    collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial
    investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on
    data collected in a small number of environments with limited scene and task diversity. In this work, we introduce
    DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or
    350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America,
    Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance,
    greater robustness, and improved generalization ability. We open source the full dataset, code for policy training,
    and a detailed guide for reproducing our robot hardware setup.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/openx.png" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="open_x_embodiment_rt_x_2023" class="col-sm-7">
        <!-- Title -->
        <div class="title">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</div>
        <!-- Author -->
        <div class="author">
        

        Open X-Embodiment Collaboration al.</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          2023
        </div>
        <div class="periodical">
          <span style="color: red;">
          
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2310.08864" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://robotics-transformer-x.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/genaug.gif" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2023genaug" class="col-sm-7">
        <!-- Title -->
        <div class="title">GenAug: Retargeting Behaviors to Unseen Situations via Generative Augmentation</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Chen</em>, Sho Kiami, Abhishek Gupta, and Vikash Kumar</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS)</em>, 2023
        </div>
        <div class="periodical">
          <span style="color: red;">
          Best System Paper Finalist
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2302.06671" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/genaug/genaug" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="/assets/pdf/genaug_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
            <a href="https://docs.google.com/presentation/d/1KJ4A6PI7hMb-ZbmH_dzrN8ORLRzIye8SOqVUM3ST8as/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
            <a href="https://genaug.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robot learning methods have the potential for widespread generalization across tasks, environments,
  and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings.
  For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience.
  In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data,
  can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data,
  they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization.
   In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation.
   By leveraging these pre-trained models for generating appropriate “semantic” data augmentations, we propose a system GenAug that is able to significantly
   improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios,
   while only requiring marginal amounts of real-world data. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/racer.jpg" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="meng2023terrainnet" class="col-sm-7">
        <!-- Title -->
        <div class="title">TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation</div>
        <!-- Author -->
        <div class="author">
        

        Xiangyun Meng, Nathan Hatch, Alexander Lambert, Anqi Li, Nolan Wagener, Matthew Schmittle, JoonHo Lee, Wentao Yuan, <em>Zoey Chen</em>, Samuel Deng, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Greg Okopal, Dieter Fox, Byron Boots, mirreza AShaban' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS)</em>, 2023
        </div>
        <div class="periodical">
          <span style="color: red;">
          
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2303.15771" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://sites.google.com/view/visual-terrain-modeling" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Effective use of camera-based vision systems is
essential for robust performance in autonomous off-road driving,
particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene
prediction have yet to be successfully adapted for complex outdoor
terrain. To this end, we present TerrainNet, a vision-based terrain
perception system for semantic and geometric terrain prediction
for aggressive, off-road navigation. The approach relies on several
key insights and practical considerations for achieving reliable
terrain modeling. The network includes a multi-headed output
representation to capture fine- and coarse-grained terrain features
necessary for estimating traversability. Accurate depth estimation
is achieved using self-supervised depth completion with multi-view
RGB and stereo inputs. Requirements for real-time performance
and fast inference speeds are met using efficient, learned image
feature projections. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/isagrasp.gif" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2022learning" class="col-sm-7">
        <!-- Title -->
        <div class="title">Learning Robust Real-world Dexterous Grasping Policies via Implicit Shape Augmentation</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Qiuyu Chen</em>, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter Fox</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Conference on Robot Learning (CoRL)</em>, 2022
        </div>
        <div class="periodical">
          <span style="color: red;">
          
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2210.13638" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://sites.google.com/view/implicitaugmentation/home" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Dexterous robotic hands have the capability to interact with a wide
variety of household objects to perform tasks like grasping. However, learning robust real world grasping policies for arbitrary objects has proven challenging due
to the difficulty of generating high quality training data. In this work, we propose
a learning system (ISAGrasp) for leveraging a small number of human demonstrations to bootstrap the generation of a much larger dataset containing successful
grasps on a variety of novel objects. Our key insight is to use a correspondenceaware implicit generative model to deform object meshes and demonstrated human grasps in order to generate a diverse dataset of novel objects and successful
grasps for supervised learning, while maintaining semantic realism. We use this
dataset to train a robust grasping policy in simulation which can be deployed in
the real world. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable="" class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dextransfer.png" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2022learninh" class="col-sm-7">
        <!-- Title -->
        <div class="title">DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Qiuyu Chen</em>, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter Fox</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS) workshop</em>, 2022
        </div>
        <div class="periodical">
          <span style="color: red;">
          Spotlight
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2209.14284" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://sites.google.com/view/dextransfer/home" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Teaching a multi-fingered dexterous robot to grasp objects in the real world has been a challenging problem due to its high dimensional state and action space. We propose a robot-learning system that can take a small number of human demonstrations and learn to grasp unseen object poses given partially occluded observations. Our system leverages a small motion capture dataset and generates a large dataset with diverse and successful trajectories for a multi-fingered robot gripper. By adding domain randomization, we show that our dataset provides robust grasping trajectories that can be transferred to a policy learner. We train a dexterous grasping policy that takes the point clouds of the object as input and predicts continuous actions to grasp objects from different initial robot states. We evaluate the effectiveness of our system on a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand with a KUKA arm in the real world. The policy learned from our dataset can generalize well on unseen object poses in both simulation and the real world.</p>
          </div>
        </div>
      </div>
</li>
</ol>

</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Zoey  Chen. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@/dist/umd/popper.min.js" integrity="" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7T85FW0C7L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7T85FW0C7L');
  </script>
  </body>
</html>

