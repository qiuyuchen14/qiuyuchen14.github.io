<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="" /><meta name="msvalidate.01" content="" />

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Zoey  Chen</title>
    <meta name="author" content="Zoey  Chen" />
    <meta name="description" content="Zoey's website.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img//assets/img/zoey.JPG"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://qiuyuchen14.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://qiuyuchen14.github.io/"><span class="font-weight-bold">Zoey</span>   Chen</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Zoey</span>  Chen
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/zoey.jpg" alt="zoey.jpg">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I received my Ph.D. in Computer Science and Engineering from the University of Washington, where I was fortunate to be advised by <a href="https://homes.cs.washington.edu/~fox/" target="_blank" rel="noopener noreferrer">Dieter Fox</a> and <a href="https://abhishekunique.github.io/" target="_blank" rel="noopener noreferrer">Abhishek Gupta</a>. 
My research interests are imitation learning and robot manipulation, with a particular focus on learning to generate diverse data for robots such that learning from limited data is possible. Prior to my PhD life, I did my master 
from Electrical Engineering Department at the University of Washington advised by <a href="https://people.ece.uw.edu/hwang/" target="_blank" rel="noopener noreferrer">Jenq-Neng Hwang</a> and <a href="http://research.nii.ac.jp/~imarik/" target="_blank" rel="noopener noreferrer">Imari Sato</a>. I am also fortunate to have interned with many awesome mentors in the past at
National Institute of Informatics (Tokyo), Microsoft Research (Redmond), NNAISENSE (Lugano), Nvidia Robotics Lab (Seattle) and Meta robotics lab (Pittsburgh).</p>

<p>In my spare time, I enjoy playing the piano, painting, movies, cooking, hiking, board games, and recently snorkeling :)</p>

<p><a href="mailto:qiuyuchen14@gmail.com">Email</a>   /  <a href="https://twitter.com/ZoeyC17" target="_blank" rel="noopener noreferrer">Twitter</a>  /  <a href="https://github.com/qiuyuchen14" target="_blank" rel="noopener noreferrer">Github</a>  /  <a href="https://linkedin.com/in/zoey-chen-01a31a200" target="_blank" rel="noopener noreferrer">LinkedIn</a>  /  <a href="https://scholar.google.com/citations?user=ZT8ib-AAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a>  /  <a href="https://www.proquest.com/openview/5bcdaaef7f8bef7898272ab8a56b58eb/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y" target="_blank" rel="noopener noreferrer">Thesis</a></p>

          </div>

          <!-- apers -->
          <div class="publications">
            <h2>Selected Publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable class="preview z-depth-1 rounded" src="/assets/img/publication_preview/rss_urdformer.gif" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2024urdformer" class="col-sm-7">
        <!-- Title -->
        <div class="title">URDFormer: Constructing Interactive Realistic Scenes from Real Images via Simulation and Generative Modeling</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Chen</em>, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Karthikeya Vemuri, Alan Wu, Dieter Fox, and Abhishek Gupta</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS)</em>, 2024
        </div>
        <div class="periodical">
          <span style="color: red;">
          Oral Presentation at CoRL TGR workshop
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2405.11656" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/WEIRDLabUW/urdformer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://urdformer.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Constructing accurate and targeted simulation scenes that are both visually and physically
  realistic is a significant practical interest in domains ranging from robotics to computer vision.
  However, this process is typically done largely by hand - a graphic designer and a simulation engineer work together
  with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
  While this may scale to small numbers of scenes, to achieve the generalization properties that are requisite of data-driven machine
  learning algorithms, we require a pipeline that is able to synthesize large numbers of realistic scenes, c
  omplete with “natural” kinematic and dynamic structure. To do so, we develop models for inferring structure and generating simulation scenes
  from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models,
  we show how effective generative models can be used in generating training data, the network can be inverted to map from realistic images back to
  complete scene models. We show how this paradigm allows us to build large datasets of scenes with semantic and physical realism,
  enabling a variety of downstream applications in robotics and computer vision. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable class="preview z-depth-1 rounded" src="/assets/img/publication_preview/genaug.gif" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2023genaug" class="col-sm-7">
        <!-- Title -->
        <div class="title">GenAug: Retargeting Behaviors to Unseen Situations via Generative Augmentation</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Chen</em>, Sho Kiami, Abhishek Gupta, and Vikash Kumar</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Robotics: Science and Systems (RSS)</em>, 2023
        </div>
        <div class="periodical">
          <span style="color: red;">
          Best System Paper Finalist
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2302.06671" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/genaug/genaug" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="/assets/pdf/genaug_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
            <a href="https://docs.google.com/presentation/d/1KJ4A6PI7hMb-ZbmH_dzrN8ORLRzIye8SOqVUM3ST8as/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
            <a href="https://genaug.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robot learning methods have the potential for widespread generalization across tasks, environments,
  and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings.
  For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience.
  In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data,
  can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data,
  they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization.
   In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation.
   By leveraging these pre-trained models for generating appropriate “semantic” data augmentations, we propose a system GenAug that is able to significantly
   improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios,
   while only requiring marginal amounts of real-world data. </p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3">
          <div style="max-width: 200px; margin: auto;">
            <img data-zoomable class="preview z-depth-1 rounded" src="/assets/img/publication_preview/isagrasp.gif" width="100%">
          </div>
        </div>


        <!-- Entry bib key -->
        <div id="chen2022learning" class="col-sm-7">
        <!-- Title -->
        <div class="title">Learning Robust Real-world Dexterous Grasping Policies via Implicit Shape Augmentation</div>
        <!-- Author -->
        <div class="author">
        

        <em>Zoey Qiuyu Chen</em>, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter Fox</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Conference on Robot Learning (CoRL)</em>, 2022
        </div>
        <div class="periodical">
          <span style="color: red;">
          
          </span>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2210.13638" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://sites.google.com/view/implicitaugmentation/home" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Dexterous robotic hands have the capability to interact with a wide
variety of household objects to perform tasks like grasping. However, learning robust real world grasping policies for arbitrary objects has proven challenging due
to the difficulty of generating high quality training data. In this work, we propose
a learning system (ISAGrasp) for leveraging a small number of human demonstrations to bootstrap the generation of a much larger dataset containing successful
grasps on a variety of novel objects. Our key insight is to use a correspondenceaware implicit generative model to deform object meshes and demonstrated human grasps in order to generate a diverse dataset of novel objects and successful
grasps for supervised learning, while maintaining semantic realism. We use this
dataset to train a robust grasping policy in simulation which can be deployed in
the real world. </p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          <div class="invited_talks">
            <h2>Invited Talks</h2>
<!--            -->
            <p>Dec 11, 2023: How to train your robot, Guest lecture at "Intro to Robotics" class at the University of Minnesota</p>
            <p>Dec 1, 2023: Learning from Imagination, Georgia Tech RoboGrads Student Seminar</p>
            <p>Nov 15, 2023: Learning to generalize with minimum demonstrations, UW Industry Day  </p>
            <p>May 30, 2023: Training robots with limited demonstrations, Tutorial at "Robotics" class at UW </p>
            <p>Mar 1, 2023: GenAug, META Research EAI Seminar </p>
            <p>Dec 1, 2022: Grasping with minimal human demonstrations, UW CSE Colloqium Robotics Research Show Case</p>
            <p>Nov 15, 2022: ISAGrasp, Industry Affiliates Research Day</p>
            <p>Aug 20, 2022: ISAGrasp, META Research EAI Seminar</p>
<!---->
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%71%69%75%79%75%63%68%65%6E%31%34@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=ZT8ib-AAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/qiuyuchen14" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://twitter.com/ZoeyC17" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            
            </div>

            <div class="contact-note">
              
            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2026 Zoey  Chen. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7T85FW0C7L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7T85FW0C7L');
  </script>
  </body>
</html>

